\section{Literature studied}

I read quite a lot, focusing particularly on papers and books relevant for the URDAD-DSL and Quality-Driven Process papers including publications 
\begin{itemize}
 \item complexity assessment of design models,
 \item model quality in general,
 \item requirements modeling and business process design,
 \item meta-modeling and domain-specific languages and concrete syntax specifications, and
  \item papers around using ontologies in modeling.
\end{itemize}

%--------------------------------------------------------------------------------------------------

\subsection{Publications around assessing design complexity including}
  \begin{itemize}
   \item \cite{mccabe_complexity_1976} which discusses a graph-theoretic approach resulting in a range of potential complexity measures including the widely used cyclometric complexity measure which evaluates the number of possible paths through a graph,
   \item \cite{tegarden_software_1995} which discuss a quasi ad-hoc parametric approach to assessing the cohesion of the individual classes as well as the cohesion between classes and base their complexity estimate for an object-oriented design largely on this cohesion measure,
   \item \cite{roca_entropy-based_1996} discusses a information theoretic approach based on the work by Shannon. They define probabilities (this part I am dubious about) around entering nodes and executing graph transitions and use those probabilities to ultimately assess the entropy which represents missing information or information which must be provided. It is this entropy which is used as complexity measure.
   \item \cite{podgorelec_estimating_2007} looks at model complexity from a general language theoretic approach, pointing out that the purpose of a model is communication. They map the string of symbols of a language (e.g. the model description) onto a Brownian motion/walk problem and estimate long range power law correlations which provide an estimate of the non-random information, hence the which contains purpose and explicit meaning. Ultimately this is used as a measure of the information content in the model which is then related to the complexity of the model.
   \item \cite{sengupta_measuring_2011} discuss a graph theoretic approach, defining a directed graph connecting caller and called components and introducing a complexity measure which contains factors estimating individual component, inter-component complexity and overall topological complexity. All these measures are largely based on inter-component interaction (calling).
   \item \cite{yadav_measuring_2009} is a largely useless paper assessing design complexity by the number of inherited methods.
   \item \cite{gill_new_2010} estimate class complexity as a sum of the complexity of all its methods. The method complexity is calculated from a number of factors including (1) the control flow complexity which is estimated using Mcabe's cyclometric complexity measure, (2) The sum of the method call complexities with the method call complexity providing a measure of the number of nodes which need to be traversed to access a method, and (3) the data call complexity which is based on the number of nodes which need to be traversed to access the required data element.
   \item \cite{braha_measurement_1998} is quite a rigorous paper which first estimates alphabet size of language by the number of operators and operands in the model and then calculate the structural information content (the size of the design). They then define a probability measure in order to be able to calculate the entropy of the design which is then directly related to the complexity of the design.
  \end{itemize}

%--------------------------------------------------------------------------------------------------

\subsection{Publications around meta-modeling and domain specific languages and concrete syntax specifications}

  \begin{itemize}
   \item \cite{osis_transforming_2010} study the possibility of transforming textual descriptions of use cases to a requirements model in the form of MDA's Computation Independent Model. This is related to our work because we do the converse of defining a formal text syntax which directly generates the requirements and process design model which is the CIM + PIM. The authors define a required structure for the textual description of a use case and then extract the topological functioning model from that structure. We, on the other hand, specify the metamodel and then a concrete syntax for our domain specific language, having thus automatically the mapping between the textual representation and the model conforming to the metamodel. They us the the Topological Functioning Model (TFM) which defines a topological space around the functional features (services) of the system and a directed graph connecting these. As such it can be related to a pure services oriented approach as represented by URDAD.
   \item \cite{hoffmann_towards_2009} define separate metamodel for narrative use case description useful for business users. Note this is not a textual syntax, but a separate metamodel.They then define a range of consistency checks between narrative and UML use case models (which we do not need). 
   \item \cite{gervais_towards_2002} defines ODAC methodology which, in turn, is based on the Open Distributed Processing (ODP) ISO standards which, in turn, is related to IEEE 1471 (architectural description) standards. This paper is relevant for us because the graphical syntax specification for URDAD will similarly be based on the concept of views and view points as specified in IEEE-1471. They also emphasize that the MDA inputs are the PIM for the technology neutral design and the PDM (Platform Description Model) which contains the specification of the target architecture and technologies and that the MDA processes generate the PSM (Platform Specific Model) and ultimately the code from these. The ODAC process is, however, very different from URDAD. The steps are to (1) define the objective, (2), Specify the role types, (3) specify the S-community which includes the specification of the secondary actors (external service providers and observers) a use case needs to interface with, (4) the identification of the interface roles which effectively is similar to the responsibility allocation step of URDAD where services are assigned to responsibility domains (roles in their case), (5) enterprise object identification (data modeling), (6) describing the community behavior which is similar to process orchestration and (7) describing the policies which covers some aspects around what needs to be done in certain circumstances and when to refuse a service. They regard the above as providing guidelines for the content of the PIM.
   \item \cite{mu_specification_2010} compare different standard meta-modeling languages and approaches including MDA's MOF, EMOF and its reference implementation Ecore, the Kernel MetaMetaModel KM3 from ATLAS INRIA and XMF-Modaic from Ceteva. XMF is inspired by MOF and OCL, but is able to describe itself. The Core language, XCore is extended with XOCL and XBNF for constraint and expression specification and XOCL is, in turn extended by XMap and XSync for specifying mappings and synchronization rules. KM3 is a textual metalanguage used to define the structure of domain specific languages (DSLs). A range of supporting languages like ATL for transformation, TCS for the specification of a concrete textual syntax and languages for view specifications are part of the KM3-based technology suite. The novelty of the paper itself is rather minimal, aiming to clear up the higher level concepts like the M-levels and the higher level architecture within which meta-modeling is done.
   \item \cite{gronback_eclipse_2009} provides a nice introductory book into domain specific languages and the tools within the Eclipse framework for defining a metamodel, defining a graphical syntax for a DSL and performing model-to-model and model-to-text transformations. I still have to read the second half of the book.
   \item \cite{lazar_realizing_2010} perform the mapping of textual use case description onto activity diagram within fUML. They aim to keep the textual use case encoding in a format which is natural and easy to understand and show some concrete examples of intuitive textual encodings. This paper provides a worth-while comparison to the textual syntax within URDAD-DSL. Note that fUML requires active, statefull classes vs URDAD's stateless services.
   \item \cite{jinhong_zhao_research_2008} define a generic, traditional OO modeling language with a textual syntax which ultimately provides an encoding which is very similar to a normal OO programming language. Not sure that this is very useful myself.
   \item \cite{mayer_mdd4soa:_2008} specify UML extension for modeling service orchestration. It is a useful reference in that we can compare their approach to URDAD. The authors also claim that the modeling approaches for SOA are still weak.
   \item \cite{mattsson_linking_2009} critically looks at a model-driven development (MDD) case study, identifying shot-comings of the MDD approach used in that study. They conclude that it is important to formalize architectural design rules. They also highlight that there are typically time-consuming error-prone manual practices required to keep system consistent with its ``architecture'' - the concept of architecture is dubious as usual. They include in their paper a nice summary statement of MDD \quote{A basic premise of MDD is to capture all important design information in a set of formal or semi-formal models}.
  \end{itemize}

%--------------------------------------------------------------------------------------------------

\subsection{Publications around requirements modeling and business process design}
  \begin{itemize}
   \item \cite{asnina_computation_2010} provide a mathematical topological model for mapping technology neutral service requirements onto available concrete services pool. I would regard this as an important paper. The authors stress the need for performing the modeling in the problem domain, discuss holistic model for CIM (single model absorbing all requirements) versus fragmented model for requirements (multiple artifacts which are not intrinsically connected) and provide a mathematical framework for performing gap analysis which identifies required services for which one cannot define a mapping onto existing services. The TFM (Topological Functioning Model) is independent of the modeling technique and can be applied to an URDAD model. Effectively also do (1) grouping of services into responsibility domains (feature sets), (2) decomposition of functional requirements across levels of granularity, and (3) orchestration of processes realizing higher level services from lower level services. Their functional properties are similar to our services contracts including inputs and outputs as well as cause and effect (lower level functional requirements = cause, post-conditions=effect). Quality requirements are not discussed. 
   \item \cite{gonzalez_unity_2009} define unity criteria which are used for various forms of modularization like grouping of services within certain classes and packaging classes into modules. This is similar to our responsibility localization approach which groups services into responsibility domains and responsibility domains within higher-level responsibility domains. They focus on trigger unity, communication unity and reaction unity. Trigger unity, if I understand it correctly, translates in our world that the information for a service request should come from a single cohesive actor unit. Communication unity represents, I think, message/data unity and reaction unity represents the unity of the process realizing the service which, in our case, is enforced by requiring that every service requires a controller which takes over the responsibility for the service and manages the control flow for the service.
   \item \cite{helming_towards_2010} introduce a UML-like diagrammatic language which they call the Unified Requirements Modeling Language (RUML). It is significantly smaller than UML and adds concepts like stakeholders, goals, requirements (functional and non-functional), services linked to requirements and services contained in services (don't like that part myself). They do not have support for either solid data structure specification or technology neutral process specification. They distinguish between solution and application domain use cases which we would argue is questionable.
   \item \cite{samir_towards_2010} discuss the structure of MDA's Computation Independent Model (CIM), claiming that the CIM = BMM + RM + BPM  (Computation Independent Model = Business Motivational Model + Requirements Model + Business Process Model). According to their paper the BMM contains the business strategies and tactics, policies and rules, goals and objectives, the RM the use cases, actors and business rules and the BPM the process specification. The separation between the CIM and the PIM is, however, at best, quite vague. The PIM is seen as refinement of CIM adding interfaces and perhaps class diagrams. The PIM is usually seen as platform independent software model, but even CIM is said by OMG to "show system in environment in which it will operate"??? In my opinion the OMG Requirements for the CIM make only partial sense. 
   \item \cite{iacob_model-driven_2008} discuss the mapping of business rules specified using OMG's {\em Semantics for Business Vocabulary and Rules} (SBVR) to service specification and orchestration, mapping onto BPEL process specifications via MDA tools.
   \item \cite{asnina_computation_2010} stress the need for performing the modeling in the problem domain as well as the need to accumulate the requirements within a single model. They effectively also group services into responsibility domains represented by their notion of feature sets, decompose functional requirements across levels of granularity, orchestrate higher level processes across lower level services and define the notion of functional with cause and effect which can be viewed as a way of specifying a services contract. In addition they provide a {\em topological functional model} (TFM) for mapping technology neutral service requirements onto available concrete services pool. The TFM is independent of the modeling technique and can be applied to an URDAD model
   \item \cite{cardei_model_2008} provide a methodology (RDDA - Requirements Driven Design Automation) for requirements specification. The resultant functional requirements can be automatically validated for consistency and completeness. They go ahead to define ODL (the OPP Design Language with OPP referring to One Pass to Production emphasizing the direct code generation. They introduce an OWL-based DSL for requirements specification. The user adds semantic descriptions inside SYS-ML. The SYSML diagrams are exported as XMI and transformed via XSLT to ODL ontology. Validation of consistency and completeness is done in Prolog against ODL. The approach is very structure focused featuring no services contracts, no recursive orchestration of higher level services from lower level services.
   \item \cite{parnas_requirements_2000} provide a simple $1\frac{1}{2}$ pager of why formalization of requirements is important.
   \item \cite{espana_evaluating_2009} compare communication analysis (particularly communicative event diagrams) to use case analysis (particularly use case diagrams) for requirements completeness, functional encapsulation completeness and linked communication completeness. They find communication analysis approach to be superior to use case approach. Speculate whether this is due to more prescriptive methodological guidance around communication analysis as compared to use case analysis. Note that the results may well have been different if they included activity or sequence diagrams with the use case specification. Very interesting and relevant for our responsibility localization step are the functional aggregation and fragmentation errors which represent incorrect responsibility groupings and insufficient levels of granularity respectively. The grouping of functional fragmentation and aggregation errors with requirements is meant to enforce cohesive functional requirements across levels of granularity. Note that the authors find that use case approach is particularly vulnerable to functional fragmentation and aggregation errors - in URDAD this is meant to be addressed by the responsibility localization step - need to assess unity criteria for this step.
  \end{itemize}

%--------------------------------------------------------------------------------------------------

\subsection{Publications around model quality in general}
  \begin{itemize}
    \item I have read parts of the PhD thesis of Chris Lange \cite{lange_christiaan_assessing_2007} and related publications \cite{lange_empirical_2004,lange_improving_2006} which takes a similar approach to us, discussing quality form the perspective of the functional and quality requirements of the design itself as specified by the different stake holders in the design, coming up with a list of quality measures which is very similar to the one we have been using. It also discusses core issues of UML including ill-defined semantics, consistency risks and complexity.
    \item \cite{mohagheghi_evaluating_2007} point out that model quality affected by modeling language, modeling process, QA techniques, experience of modelers and modeling tools. They state that the quality goals according to SYSML are correctness, precision, conciseness, consistency and understandability. The quality requirements as identified in this paper are once again very similar to what we have been using including modifiability, testability, understandability, complexity, balance, modularity, self-descriptiveness, conciseness, precision, aesthetics detailedness, consistency and completeness. There is quite a bit of overlap between these, but still a useful reference. They also point out that the complexity of modeling language determined by complexity of metamodel.
    \item \cite{ramesh_toward_2001} study requirements traceability. The paper is relevant to assess the traceability supported by the URDAD metamodel. They identify 4 core traceability link types including (1) \emph{satisfaction links} ensuring that requirements are satisfied by system - checks/ensures, (2) \emph{evolutionary links} which document inputs resulting in changes to objects and resulting changes, (3) \emph{rationale links} which provide linkage to the rationale of why something is why is this required and (4) \emph{dependency links} which document dependencies between model elements. In the URDAD metamodel satisfaction links are represented by \verb+usedToAddress+ links between services and pre- and post-conditions. Evolutionary links are not addressed within the URDAD metamodel and are left to the version control environment. Higher level purpose, goals and rationale links are also not addressed in URDAD. The metamodel does, however, include the \verb+requiredBy+ linkage between requirements and the stakeholders which require them. A requirement around a service may be required by a responsibility domain (i.e.\ a roles) or by another service. The metamodel enforces that the traceability of a requirement and the responsibility domains/services for which require them is specified. Finally, dependency links are included in the model. The dependency is foremost between services and service contracts, i.e.\ a particular process design for a service has dependencies on services contracts (not on particular service implementations). Note that the URDAD metamodel does not differentiate between dependency and satisfaction links. Requirements result in dependencies and if the dependencies are available, the requirements are satisfied.
    \item \cite{aizenbud-reshef_model_2006} discuss model traceability, emphasizing that maintenance of the traceability links is cumbersome and error-prone and point out that a lot of the traceability links can be auto-generated from the model. This is aligned with URDAD where all dependency links, and evolutionary links are not explicitly specified but links which contain extra information (requiredBy and usedToAddress) are explicitly specified in the model.
    \item \cite{khoshkbarforoushha_metric_2010} is not a very substantial paper. They study mismatch of services preventing their reuse and predictably identify the following causes of service mismatch: (1) requirements mismatch (that a service does not fulfill the requirements) and (2) description mismatch (that s service does not implement the required WSDL).
  \item \cite{bashardoust-tajali_extracting_2008} stress the need for testable domain models. They do not define up-front contracts with formally specified via pre/post-conditions, but extract the contracts themselves from the information of the domain/requirements model. Ultimately they perform test extraction from generated contracts.

  \end{itemize}

%--------------------------------------------------------------------------------------------------

\subsection{Papers around using ontologies in modeling}  

  \begin{itemize}
   \item \cite{staab_model_2010}, in this important paper, point out that many of the meta-models used as a basis for software models do not have a formally specified or consistent semantics. They go ahead to define semantic and syntactic constraints on abstract syntax for meta-models. They emphasize that the semantics should be defined either through precise metamodel or through transformation into a logic representation. They also stress that model correctness is often checked via through informal analysis, whilst it should be made explicit through constraints on the metamodel. Once a metamodel has been transformed into a logic representation (e.g. OWL-DL) one can use standard ontology reasoning services to assess (1) consistency (if instance of ontology can exist), (2) satisfiable (all concepts/classes are satisfiable), (3)  classification (returns for an individual a set of concepts within ontology which contain/describe individual), and (4) subsumption (checks whether individuals of class A are subset of individuals of class B). Like us, they perform the mapping between MOF and OWL spaces using \verb+twoUse+.
   \item \cite{henderson-sellers_bridging_2011} provide a literature review of the interface between ontologies and meta-modeling. They stress the equivalence of meta-modeling with meta-ontologies. Need to read up on that topic.
   \item \cite{wang_ontology_2006} transform UML to OWL models and use logical reasoning for consistency checking. They provide a nice diagram showing UML vs DSL based MDA. The authors stress that UML models are often inconsistent and incomplete and that hence the management of both, syntactic and semantic consistency is key for UML modeling.
   \item \cite{hemingway_semantic_2007} perform model-driven development for embedded systems using semantic technologies. They point out that  DSMLs (domain specific modeling language)  are commonly used for embedded systems development and that such languages are provided by a wide range of products. They discuss what they call semantic anchoring to facilitate mapping between DSLs
  \end{itemize}
